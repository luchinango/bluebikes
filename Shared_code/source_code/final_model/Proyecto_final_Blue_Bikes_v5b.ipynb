{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["FsuxfpCRUYvP","yWYA-9QM3fTS"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Cargado de archivos**"],"metadata":{"id":"FsuxfpCRUYvP"}},{"cell_type":"code","source":["# =============================================================================\n","# Configuración optimizada de la sesión de Spark\n","# =============================================================================\n","# Esta configuración es más robusta para conjuntos de datos grandes.\n","# Debe ejecutarse una vez al inicio del notebook.\n","\n","from pyspark.sql import SparkSession\n","import os\n","\n","spark = SparkSession.builder \\\n","    .appName(\"BlueBikes-Project\") \\\n","    .config(\"spark.driver.memory\", \"8g\") \\\n","    .config(\"spark.executor.memory\", \"8g\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n","    .config(\"spark.network.timeout\", \"800s\") \\\n","    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n","    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n","    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Establecer variables de entorno para que PySpark funcione correctamente en algunos entornos\n","import sys\n","# os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n","# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n","os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n","os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\""],"metadata":{"id":"eJFdZIdtnr90","executionInfo":{"status":"ok","timestamp":1755024019928,"user_tz":240,"elapsed":62,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# =============================================================================\n","# Cargar DataFrame procesado desde Google Drive (para Colab)\n","# =============================================================================\n","# Esta celda activa tu Google Drive y lee los datos procesados del archivo Parquet almacenado allí.\n","# Usarlo en el entorno de Google Colab.\n","\n","from google.colab import drive\n","\n","print(\"Intentando montar Google Drive...\")\n","try:\n","    drive.mount('/content/drive')\n","\n","    gdrive_path = \"/content/drive/MyDrive/DM_PRJ/df_final_bluebikes_v2.parquet\"\n","    print(f\"Cargando datos desde la ruta de Google Drive: {gdrive_path}\")\n","\n","    # Spark leerá la carpeta Parquet directamente\n","    df_final = spark.read.parquet(gdrive_path)\n","\n","    print(\"✅ DataFrame cargado exitosamente desde Google Drive.\")\n","\n","    # Verify the schema and show a few rows\n","    print(\"Esquema de DataFrame:\")\n","    df_final.printSchema()\n","\n","    print(\"Muestra de los datos cargados:\")\n","    df_final.show(5, truncate=False)\n","\n","except Exception as e:\n","    print(f\"❌ Error al cargar datos desde Google Drive. Asegúrate de que el archivo exista en '{gdrive_path}'.\")\n","    print(f\"Detalles del error: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fp_97YheUd1j","executionInfo":{"status":"ok","timestamp":1755024229763,"user_tz":240,"elapsed":209837,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"outputId":"0326e9cc-fbd1-4252-a670-10756132461a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Intentando montar Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Cargando datos desde la ruta de Google Drive: /content/drive/MyDrive/DM_PRJ/df_final_bluebikes_v2.parquet\n","✅ DataFrame cargado exitosamente desde Google Drive.\n","Esquema de DataFrame:\n","root\n"," |-- start_station_id: string (nullable = true)\n"," |-- ride_id: string (nullable = true)\n"," |-- started_at: timestamp (nullable = true)\n"," |-- ended_at: timestamp (nullable = true)\n"," |-- start_station_name: string (nullable = true)\n"," |-- start_lat: double (nullable = true)\n"," |-- start_lng: double (nullable = true)\n"," |-- end_station_id: string (nullable = true)\n"," |-- end_station_name: string (nullable = true)\n"," |-- end_lat: double (nullable = true)\n"," |-- end_lng: double (nullable = true)\n"," |-- member_casual: string (nullable = true)\n"," |-- duration_sec: long (nullable = true)\n"," |-- schema_version: string (nullable = true)\n"," |-- periodo: string (nullable = true)\n"," |-- log_duration: double (nullable = true)\n"," |-- trip_year: integer (nullable = true)\n"," |-- trip_month: integer (nullable = true)\n"," |-- trip_day_of_week: integer (nullable = true)\n"," |-- trip_hour: integer (nullable = true)\n"," |-- is_weekend: integer (nullable = true)\n"," |-- season: string (nullable = true)\n"," |-- hour_sin: double (nullable = true)\n"," |-- hour_cos: double (nullable = true)\n"," |-- day_of_week_sin: double (nullable = true)\n"," |-- day_of_week_cos: double (nullable = true)\n"," |-- haversine_distance_km: double (nullable = true)\n"," |-- is_popular_start: integer (nullable = true)\n"," |-- avg_speed_kmh: double (nullable = true)\n"," |-- is_round_trip: integer (nullable = true)\n","\n","Muestra de los datos cargados:\n","+----------------+-------+-----------------------+-----------------------+------------------------+---------+---------+--------------+---------------------------+-----------------+------------------+-------------+------------+--------------+-------+-----------------+---------+----------+----------------+---------+----------+------+------------------+--------------------+-------------------+-------------------+---------------------+----------------+------------------+-------------+\n","|start_station_id|ride_id|started_at             |ended_at               |start_station_name      |start_lat|start_lng|end_station_id|end_station_name           |end_lat          |end_lng           |member_casual|duration_sec|schema_version|periodo|log_duration     |trip_year|trip_month|trip_day_of_week|trip_hour|is_weekend|season|hour_sin          |hour_cos            |day_of_week_sin    |day_of_week_cos    |haversine_distance_km|is_popular_start|avg_speed_kmh     |is_round_trip|\n","+----------------+-------+-----------------------+-----------------------+------------------------+---------+---------+--------------+---------------------------+-----------------+------------------+-------------+------------+--------------+-------+-----------------+---------+----------+----------------+---------+----------+------+------------------+--------------------+-------------------+-------------------+---------------------+----------------+------------------+-------------+\n","|296             |4512   |2019-08-01 05:40:03.11 |2019-08-01 05:53:58.066|Farragut Rd at E. 6th St|42.3334  |-71.02495|48            |Post Office Square         |42.35585435522005|-71.05459745998814|member       |835         |schema1       |201908 |6.727431724850856|2019     |8         |5               |5        |0         |summer|0.9659258262890683|0.25881904510252074 |-0.9749279121818236|-0.2225209339563146|3.4886894424332344   |0               |15.040991431365857|0            |\n","|296             |5426   |2019-08-01 07:09:28.008|2019-08-01 07:28:09.388|Farragut Rd at E. 6th St|42.3334  |-71.02495|43            |Rowes Wharf at Atlantic Ave|42.357143        |-71.050699        |member       |1121        |schema1       |201908 |7.02197642307216 |2019     |8         |5               |7        |0         |summer|0.9659258262890683|-0.25881904510252063|-0.9749279121818236|-0.2225209339563146|3.383527269533858    |0               |10.865886755690962|0            |\n","|296             |4381   |2019-08-01 07:40:53.75 |2019-08-01 07:51:35.875|Farragut Rd at E. 6th St|42.3334  |-71.02495|135           |ID Building East           |42.344827        |-71.028664        |member       |642         |schema1       |201908 |6.464588303689961|2019     |8         |5               |7        |0         |summer|0.9659258262890683|-0.25881904510252063|-0.9749279121818236|-0.2225209339563146|1.3067788774106672   |0               |7.327690933007855 |0            |\n","|296             |2152   |2019-08-01 08:11:56.553|2019-08-01 08:47:36.94 |Farragut Rd at E. 6th St|42.3334  |-71.02495|163           |The Lawn on D              |42.344792        |-71.044024        |member       |2140        |schema1       |201908 |7.668561108015897|2019     |8         |5               |8        |0         |summer|0.8660254037844387|-0.4999999999999998 |-0.9749279121818236|-0.2225209339563146|2.0155391192751053   |0               |3.3906208519417347|0            |\n","|296             |4109   |2019-08-01 08:41:19.422|2019-08-01 09:03:22.734|Farragut Rd at E. 6th St|42.3334  |-71.02495|50            |Boylston St at Berkeley St |42.3511419825475 |-71.07329249382019|member       |1323        |schema1       |201908 |7.187657164114956|2019     |8         |5               |8        |0         |summer|0.8660254037844387|-0.4999999999999998 |-0.9749279121818236|-0.2225209339563146|4.43600172657544     |0               |12.07072015198718 |0            |\n","+----------------+-------+-----------------------+-----------------------+------------------------+---------+---------+--------------+---------------------------+-----------------+------------------+-------------+------------+--------------+-------+-----------------+---------+----------+----------------+---------+----------+------+------------------+--------------------+-------------------+-------------------+---------------------+----------------+------------------+-------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["# **Evaluación de modelo**"],"metadata":{"id":"yWYA-9QM3fTS"}},{"cell_type":"code","source":["# =============================================================================\n","# Selección avanzada de funciones (con conjunto de características refinado)\n","# =============================================================================\n","\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.sql.functions import col\n","import pandas as pd\n","\n","# --- Paso 1: Preparación de datos ---\n","\n","# Realizar un muestreo estratificado en la columna de cadena original.\n","label_col = 'member_casual'\n","distinct_labels = [row[label_col] for row in df_final.select(label_col).distinct().collect()]\n","fractions = {label: 0.5 for label in distinct_labels} # Usar una muestra del 50%\n","\n","# Crear la muestra estratificada y divídala en conjuntos de entrenamiento y prueba\n","df_sample = df_final.sampleBy(label_col, fractions=fractions, seed=42)\n","train_data, test_data = df_sample.randomSplit([0.8, 0.2], seed=42)\n","\n","# Almacenar en caché los datos de entrenamiento para un acceso más rápido\n","train_data.cache().count()\n","\n","# --- Paso 2: Definir el pipeline completo ---\n","\n","# 1. Indexar la columna de etiquetas\n","label_indexer = StringIndexer(inputCol=label_col, outputCol='label', handleInvalid='keep')\n","\n","# 2. CAMBIO: Actualizar la lista de características numéricas\n","#    - Se AÑADEN 'avg_speed_kmh' y 'is_round_trip'.\n","#    - Se ELIMINAN 'trip_hour' y 'trip_day_of_week' para evitar redundancia con las versiones sin/cos.\n","categorical_features = ['season']\n","numerical_features = [\n","    'log_duration', 'haversine_distance_km', 'is_popular_start',\n","    'trip_month',\n","    'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos',\n","    'avg_speed_kmh', 'is_round_trip' # <-- NUEVAS CARACTERÍSTICAS\n","]\n","\n","# 3. Crear etapas para la codificación de características categóricas\n","stages = [label_indexer]\n","encoded_feature_names = []\n","for c in categorical_features:\n","    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\", handleInvalid='keep')\n","    encoder = OneHotEncoder(inputCol=f\"{c}_indexed\", outputCol=f\"{c}_encoded\", dropLast=False)\n","    stages.extend([indexer, encoder])\n","    encoded_feature_names.append(f\"{c}_encoded\")\n","\n","# 4. Ensamblar todas las características en un solo vector\n","all_assembler_cols = numerical_features + encoded_feature_names\n","assembler = VectorAssembler(inputCols=all_assembler_cols, outputCol=\"features\")\n","stages.append(assembler)\n","\n","# 5. Definir el modelo RandomForest para obtener la importancia de las características\n","rf = RandomForestClassifier(labelCol='label', featuresCol=\"features\", numTrees=100, seed=42)\n","stages.append(rf)\n","\n","# Crear el objeto de pipeline completo\n","pipeline = Pipeline(stages=stages)\n","\n","# --- Paso 3: Entrenar el modelo y extraer la importancia de las características ---\n","\n","model = pipeline.fit(train_data)\n","\n","# Extraer puntuaciones de importancia\n","importances = model.stages[-1].featureImportances.toArray()\n","\n","# Obtener los nombres de las características\n","feature_names = model.stages[-2].getInputCols()\n","\n","# Crear un DataFrame para mostrar la importancia de las características\n","importance_df = pd.DataFrame(list(zip(feature_names, importances)), columns=['feature', 'importance'])\n","importance_df = importance_df.sort_values('importance', ascending=False)\n","\n","print(\"Las 10 características principales por importancia (con el nuevo conjunto de características):\")\n","print(importance_df.head(10))\n","\n","# Seleccionar dinámicamente las 10 características principales para el modelo final\n","top_features = importance_df.head(10)['feature'].tolist()\n","print(f\"\\nCaracterísticas seleccionadas para el modelo final:\\n{top_features}\")"],"metadata":{"id":"bXZsLIZLH5qD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755046257312,"user_tz":240,"elapsed":2500269,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"outputId":"a4c0f113-02b3-45af-a0a9-feca10561da9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Las 10 características principales por importancia (con el nuevo conjunto de características):\n","                  feature  importance\n","8           avg_speed_kmh    0.408620\n","0            log_duration    0.275662\n","7         day_of_week_cos    0.138945\n","9           is_round_trip    0.065822\n","1   haversine_distance_km    0.058860\n","3              trip_month    0.008754\n","4                hour_sin    0.006530\n","10         season_encoded    0.005388\n","2        is_popular_start    0.005151\n","6         day_of_week_sin    0.004390\n","\n","Características seleccionadas para el modelo final:\n","['avg_speed_kmh', 'log_duration', 'day_of_week_cos', 'is_round_trip', 'haversine_distance_km', 'trip_month', 'hour_sin', 'season_encoded', 'is_popular_start', 'day_of_week_sin']\n"]}]},{"cell_type":"code","source":["# ============================================================================================\n","# MOdelo Final: GBTClassifier con Hyperparameter Tuning utilizando CrossValidator\n","# ============================================================================================\n","# Este es el bloque final de modelado. Se utiliza un CrossValidator para encontrar los mejores\n","# hiperparámetros para el modelo GBT, asegurando el máximo rendimiento posible.\n","\n","# CAMBIO: Importar las herramientas necesarias para la validación cruzada\n","from pyspark.ml.classification import GBTClassifier\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml import Pipeline, PipelineModel\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import pandas as pd\n","from pyspark.sql.functions import when, col\n","\n","# --- Paso 1: Preparar datos (sin cambios) ---\n","feature_transformer_pipeline = PipelineModel(stages=model.stages[:-1])\n","transformed_train_data = feature_transformer_pipeline.transform(train_data)\n","transformed_test_data = feature_transformer_pipeline.transform(test_data)\n","\n","# --- Paso 2: Calcular y aplicar pesos de clase (sin cambios) ---\n","total_count = train_data.count()\n","casual_count = train_data.filter(col(\"member_casual\") == \"casual\").count()\n","member_count = total_count - casual_count\n","weight_casual = total_count / (2.0 * casual_count)\n","weight_member = total_count / (2.0 * member_count)\n","print(f\"Peso calculado para la clase 'member': {weight_member:.2f}\")\n","print(f\"Peso calculado para la clase 'casual': {weight_casual:.2f}\\n\")\n","weighted_train_data = transformed_train_data.withColumn(\n","    \"class_weight\",\n","    when(col(\"member_casual\") == \"casual\", weight_casual)\n","    .otherwise(weight_member)\n",")\n","\n","# --- Paso 3: Configurar el Pipeline de Afinamiento (Hyperparameter Tuning) ---\n","print(f\"Reconstruyendo el pipeline para afinamiento con las siguientes características:\\n {top_features}\\n\")\n","\n","# 1. Ensamblador de características (sin cambios)\n","final_assembler_cols = top_features\n","assembler_final = VectorAssembler(inputCols=final_assembler_cols, outputCol=\"final_features\")\n","\n","# 2. Definir el modelo base GBTClassifier\n","gbt = GBTClassifier(\n","    labelCol=\"label\",\n","    featuresCol=\"final_features\",\n","    weightCol=\"class_weight\",\n","    seed=42\n",")\n","\n","# 3. CAMBIO: Construir una grilla de hiperparámetros para probar\n","paramGrid = (ParamGridBuilder()\n","             .addGrid(gbt.maxDepth, [3, 5])      # Profundidad de los árboles\n","             .addGrid(gbt.maxIter, [20, 40])     # Número de árboles\n","             .build())\n","\n","# 4. CAMBIO: Usar un evaluador binario, más adecuado para problemas de dos clases (member/casual)\n","#    La métrica 'areaUnderPR' (Área bajo la curva de Precisión-Recall) es excelente para datos desbalanceados.\n","evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderPR')\n","\n","# 5. CAMBIO: Configurar el CrossValidator\n","#    Este se encargará de entrenar y evaluar múltiples versiones del modelo para encontrar la mejor.\n","crossval = CrossValidator(estimator=gbt,\n","                          estimatorParamMaps=paramGrid,\n","                          evaluator=evaluator,\n","                          numFolds=3)  # Usar 3 divisiones para un balance entre robustez y tiempo de ejecución\n","\n","# 6. Crear el pipeline final. Su última etapa es el CrossValidator.\n","pipeline_final = Pipeline(stages=[assembler_final, crossval])\n","\n","\n","# --- Paso 4: Entrenamiento y Evaluación ---\n","# ¡Este paso ahora ejecutará todo el proceso de validación cruzada!\n","print(\"Iniciando el proceso de afinamiento con CrossValidator... Esto puede tardar varios minutos.\")\n","final_model = pipeline_final.fit(weighted_train_data)\n","print(\"Afinamiento completado.\")\n","\n","# Realizar predicciones usando el mejor modelo encontrado por el CrossValidator\n","predictions = final_model.transform(transformed_test_data)\n","\n","\n","# --- Paso 5: Métricas de Rendimiento del Mejor Modelo ---\n","evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n","accuracy = evaluator_accuracy.evaluate(predictions)\n","\n","print(f\"\\nNueva precisión del modelo (GBT Afinaddo): {accuracy:.4f}\")\n","print(f\"Precisión anterior (GBT sin afinar): 0.6831\")\n","improvement = accuracy - 0.6831\n","print(f\"Mejora del modelo: {improvement:+.4f}\\n\")\n","\n","# Get the label mapping from the trained pipeline model\n","# Access the StringIndexer model from the trained pipeline model\n","label_indexer_model = model.stages[0]\n","# Get the mapping of original labels to indexed labels\n","label_map = {index: label for index, label in enumerate(label_indexer_model.labels)}\n","\n","# Métricas de la matriz de confusión\n","predictionAndLabels = predictions.select('prediction', 'label').rdd.map(lambda r: (float(r.prediction), float(r.label)))\n","metrics = MulticlassMetrics(predictionAndLabels)\n","\n","confusion_matrix = metrics.confusionMatrix().toArray()\n","class_labels = model.stages[0].labels\n","report_df = pd.DataFrame(confusion_matrix,\n","                         index=[f'Actual: {l}' for l in class_labels],\n","                         columns=[f'Predicted: {l}' for l in class_labels])\n","print(\"Matriz de confusión del mejor modelo:\")\n","print(report_df)\n","\n","# Utilizar label_map para obtener el índice numérico de 'member'\n","positive_class_label = float([k for k, v in label_map.items() if v == 'member'][0])\n","print(\"\\nInforme de clasificación del mejor modelo (para la clase 'member'):\")\n","print(f\"Precision: {metrics.precision(positive_class_label):.4f}\")\n","print(f\"Recall: {metrics.recall(positive_class_label):.4f}\")\n","print(f\"F1-Score: {metrics.fMeasure(positive_class_label):.4f}\\n\")\n","\n","train_data.unpersist()"],"metadata":{"id":"z2Pz2ixRefAT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755058670273,"user_tz":240,"elapsed":12412966,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"outputId":"e50266ee-175a-498a-9abb-c2b0971d6683"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Peso calculado para la clase 'member': 0.64\n","Peso calculado para la clase 'casual': 2.28\n","\n","Reconstruyendo el pipeline para afinamiento con las siguientes características:\n"," ['avg_speed_kmh', 'log_duration', 'day_of_week_cos', 'is_round_trip', 'haversine_distance_km', 'trip_month', 'hour_sin', 'season_encoded', 'is_popular_start', 'day_of_week_sin']\n","\n","Iniciando el proceso de afinamiento con CrossValidator... Esto puede tardar varios minutos.\n","Afinamiento completado.\n","\n","Nueva precisión del modelo (GBT Afinaddo): 0.6866\n","Precisión anterior (GBT sin afinar): 0.6831\n","Mejora del modelo: +0.0035\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Matriz de confusión del mejor modelo:\n","                Predicted: member  Predicted: casual\n","Actual: member          1353461.0           564061.0\n","Actual: casual           205319.0           332351.0\n","\n","Informe de clasificación del mejor modelo (para la clase 'member'):\n","Precision: 0.8683\n","Recall: 0.7058\n","F1-Score: 0.7787\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["DataFrame[start_station_id: string, ride_id: string, started_at: timestamp, ended_at: timestamp, start_station_name: string, start_lat: double, start_lng: double, end_station_id: string, end_station_name: string, end_lat: double, end_lng: double, member_casual: string, duration_sec: bigint, schema_version: string, periodo: string, log_duration: double, trip_year: int, trip_month: int, trip_day_of_week: int, trip_hour: int, is_weekend: int, season: string, hour_sin: double, hour_cos: double, day_of_week_sin: double, day_of_week_cos: double, haversine_distance_km: double, is_popular_start: int, avg_speed_kmh: double, is_round_trip: int]"]},"metadata":{},"execution_count":11}]}]}