{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Cargado de archivos**"],"metadata":{"id":"FsuxfpCRUYvP"}},{"cell_type":"code","source":["# =============================================================================\n","# Configuración optimizada de la sesión de Spark\n","# =============================================================================\n","# Esta configuración es más robusta para conjuntos de datos grandes.\n","# Debe ejecutarse una vez al inicio del notebook.\n","\n","from pyspark.sql import SparkSession\n","import os\n","\n","spark = SparkSession.builder \\\n","    .appName(\"BlueBikes-Project\") \\\n","    .config(\"spark.driver.memory\", \"8g\") \\\n","    .config(\"spark.executor.memory\", \"8g\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n","    .config(\"spark.network.timeout\", \"800s\") \\\n","    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n","    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n","    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Establecer variables de entorno para que PySpark funcione correctamente en algunos entornos\n","import sys\n","# os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n","# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n","os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n","os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\""],"metadata":{"id":"eJFdZIdtnr90","executionInfo":{"status":"ok","timestamp":1754756374574,"user_tz":240,"elapsed":15970,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# =============================================================================\n","# Cargar DataFrame procesado desde Google Drive (para Colab)\n","# =============================================================================\n","# Esta celda activa tu Google Drive y lee los datos procesados del archivo Parquet almacenado allí.\n","# Usarlo en el entorno de Google Colab.\n","\n","from google.colab import drive\n","\n","print(\"Attempting to mount Google Drive...\")\n","try:\n","    drive.mount('/content/drive')\n","\n","    gdrive_path = \"/content/drive/MyDrive/DM_PRJ/df_final_bluebikes.parquet\"\n","    print(f\"Cargando datos desde la ruta de Google Drive: {gdrive_path}\")\n","\n","    # Spark leerá la carpeta Parquet directamente\n","    df_final = spark.read.parquet(gdrive_path)\n","\n","    print(\"✅ DataFrame cargado exitosamente desde Google Drive.\")\n","\n","    # Verify the schema and show a few rows\n","    print(\"Esquema de DataFrame:\")\n","    df_final.printSchema()\n","\n","    print(\"Muestra de los datos cargados:\")\n","    df_final.show(5, truncate=False)\n","\n","except Exception as e:\n","    print(f\"❌ Error al cargar datos desde Google Drive. Asegúrate de que el archivo exista en '{gdrive_path}'.\")\n","    print(f\"Detalles del error: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fp_97YheUd1j","executionInfo":{"status":"ok","timestamp":1754756427339,"user_tz":240,"elapsed":52762,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"outputId":"60459a28-98e8-47ee-c858-e851722fbbdf"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to mount Google Drive...\n","Mounted at /content/drive\n","Cargando datos desde la ruta de Google Drive: /content/drive/MyDrive/DM_PRJ/df_final_bluebikes.parquet\n","✅ DataFrame cargado exitosamente desde Google Drive.\n","Esquema de DataFrame:\n","root\n"," |-- start_station_id: string (nullable = true)\n"," |-- ride_id: string (nullable = true)\n"," |-- started_at: timestamp (nullable = true)\n"," |-- ended_at: timestamp (nullable = true)\n"," |-- start_station_name: string (nullable = true)\n"," |-- start_lat: double (nullable = true)\n"," |-- start_lng: double (nullable = true)\n"," |-- end_station_id: string (nullable = true)\n"," |-- end_station_name: string (nullable = true)\n"," |-- end_lat: double (nullable = true)\n"," |-- end_lng: double (nullable = true)\n"," |-- member_casual: string (nullable = true)\n"," |-- duration_sec: long (nullable = true)\n"," |-- schema_version: string (nullable = true)\n"," |-- periodo: string (nullable = true)\n"," |-- log_duration: double (nullable = true)\n"," |-- trip_year: integer (nullable = true)\n"," |-- trip_month: integer (nullable = true)\n"," |-- trip_day_of_week: integer (nullable = true)\n"," |-- trip_hour: integer (nullable = true)\n"," |-- is_weekend: integer (nullable = true)\n"," |-- season: string (nullable = true)\n"," |-- hour_sin: double (nullable = true)\n"," |-- hour_cos: double (nullable = true)\n"," |-- day_of_week_sin: double (nullable = true)\n"," |-- day_of_week_cos: double (nullable = true)\n"," |-- haversine_distance_km: double (nullable = true)\n"," |-- is_popular_start: integer (nullable = true)\n","\n","Muestra de los datos cargados:\n","+----------------+----------------+-------------------+-------------------+----------------------+-----------------+----------------+--------------+---------------------------------------------------------+------------------+------------------+-------------+------------+--------------+-------+------------------+---------+----------+----------------+---------+----------+------+----------------------+-------------------+-----------------------+------------------+---------------------+----------------+\n","|start_station_id|ride_id         |started_at         |ended_at           |start_station_name    |start_lat        |start_lng       |end_station_id|end_station_name                                         |end_lat           |end_lng           |member_casual|duration_sec|schema_version|periodo|log_duration      |trip_year|trip_month|trip_day_of_week|trip_hour|is_weekend|season|hour_sin              |hour_cos           |day_of_week_sin        |day_of_week_cos   |haversine_distance_km|is_popular_start|\n","+----------------+----------------+-------------------+-------------------+----------------------+-----------------+----------------+--------------+---------------------------------------------------------+------------------+------------------+-------------+------------+--------------+-------+------------------+---------+----------+----------------+---------+----------+------+----------------------+-------------------+-----------------------+------------------+---------------------+----------------+\n","|M32051          |AE28B9F4A8213348|2023-12-12 10:25:50|2023-12-12 10:34:38|Fresh Pond Reservation|42.38267827521855|-71.143478951426|M32084        |87-101 Cambridgepark Drive                               |42.395221870310664|-71.14421904097979|member       |528         |schema2       |202312 |6.269096283706261 |2023     |12        |3               |10       |0         |winter|0.49999999999999994   |-0.8660254037844387|0.43388373911755823    |-0.900968867902419|1.3961078583530382   |0               |\n","|M32051          |75DAE72212E78D33|2023-12-19 11:08:42|2023-12-19 11:42:20|Fresh Pond Reservation|42.38267827521855|-71.143478951426|D32023        |Spaulding Rehabilitation Hospital - Charlestown Navy Yard|42.378338         |-71.048927        |member       |2018        |schema2       |202312 |7.609862200913554 |2023     |12        |3               |11       |0         |winter|0.258819045102521     |-0.9659258262890682|0.43388373911755823    |-0.900968867902419|7.781287880652623    |0               |\n","|M32051          |E401555BB0C200CF|2023-12-02 15:23:06|2023-12-02 15:35:07|Fresh Pond Reservation|42.382678        |-71.143479      |M32029        |Porter Square Station                                    |42.387995         |-71.119084        |casual       |721         |schema2       |202312 |6.580639137284949 |2023     |12        |7               |15       |1         |winter|-0.7071067811865471   |-0.7071067811865479|-2.4492935982947064E-16|1.0               |2.0890109645785846   |0               |\n","|M32051          |3B0E30D14AEE5EFD|2023-12-09 15:08:01|2023-12-09 15:42:37|Fresh Pond Reservation|42.38267827521855|-71.143478951426|M32075        |Danehy Park at New Street                                |42.389456         |-71.139019        |member       |2076        |schema2       |202312 |7.638198244285779 |2023     |12        |7               |15       |1         |winter|-0.7071067811865471   |-0.7071067811865479|-2.4492935982947064E-16|1.0               |0.8379505912305671   |0               |\n","|M32051          |A49DF27799AA5BEE|2023-12-24 12:30:08|2023-12-24 12:35:30|Fresh Pond Reservation|42.38267827521855|-71.143478951426|M32075        |Danehy Park at New Street                                |42.389456         |-71.139019        |member       |322         |schema2       |202312 |5.7745515455444085|2023     |12        |1               |12       |1         |winter|1.2246467991473532E-16|-1.0               |0.7818314824680298     |0.6234898018587336|0.8379505912305671   |0               |\n","+----------------+----------------+-------------------+-------------------+----------------------+-----------------+----------------+--------------+---------------------------------------------------------+------------------+------------------+-------------+------------+--------------+-------+------------------+---------+----------+----------------+---------+----------+------+----------------------+-------------------+-----------------------+------------------+---------------------+----------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["# **Evaluación de modelo**"],"metadata":{"id":"yWYA-9QM3fTS"}},{"cell_type":"code","source":["# =============================================================================\n","# Selección avanzada de funciones\n","# =============================================================================\n","\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.sql.functions import col\n","import pandas as pd\n","\n","# --- Paso 1: Preparación de datos ---\n","\n","# Realizar un muestreo estratificado en la columna de cadena original.\n","# Primero, buscar los distintos tipos de usuario para crear el diccionario de fracciones.\n","label_col = 'member_casual'\n","distinct_labels = [row[label_col] for row in df_final.select(label_col).distinct().collect()]\n","fractions = {label: 0.5 for label in distinct_labels} # % de muestra para cada tipo de usuario\n","\n","# Crear la muestra estratificada y divídala en conjuntos de entrenamiento y prueba\n","df_sample = df_final.sampleBy(label_col, fractions=fractions, seed=42)\n","train_data, test_data = df_sample.randomSplit([0.8, 0.2], seed=42)\n","\n","# Almacenar en caché los datos de entrenamiento para un acceso más rápido durante el entrenamiento del modelo\n","train_data.cache().count()\n","\n","# --- Paso 2: Definir el pipeline completo ---\n","\n","# El pipeline ahora gestionará todas las transformaciones, incluida la indexación de etiquetas.\n","# Esto garantiza que se ajuste a los datos que aún no tienen una columna de \"label\".\n","\n","# 1. Indexar la columna de etiquetas\n","label_indexer = StringIndexer(inputCol=label_col, outputCol='label', handleInvalid='keep')\n","\n","# 2. Definir características categóricas y numéricas\n","categorical_features = ['season']\n","numerical_features = [\n","    'log_duration', 'haversine_distance_km', 'is_popular_start',\n","    'trip_hour', 'trip_day_of_week', 'trip_month',\n","    'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos'\n","]\n","\n","# 3. Crear etapas para la codificación de características categóricas\n","stages = [label_indexer]\n","encoded_feature_names = []\n","for c in categorical_features:\n","    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\", handleInvalid='keep')\n","    encoder = OneHotEncoder(inputCol=f\"{c}_indexed\", outputCol=f\"{c}_encoded\", dropLast=False)\n","    stages.extend([indexer, encoder])\n","    encoded_feature_names.append(f\"{c}_encoded\")\n","\n","# 4. Ensamblar todas las características en un solo vector\n","all_assembler_cols = numerical_features + encoded_feature_names\n","assembler = VectorAssembler(inputCols=all_assembler_cols, outputCol=\"features\")\n","stages.append(assembler)\n","\n","# 5. Definir el modelo RandomForest para obtener la importancia de las características\n","rf = RandomForestClassifier(labelCol='label', featuresCol=\"features\", numTrees=100, seed=42)\n","stages.append(rf)\n","\n","# Crear el objeto de pipeline completo\n","pipeline = Pipeline(stages=stages)\n","\n","# --- Paso 3: Entrenar el modelo y extraer la importancia de las características ---\n","\n","# Entrenar el modelo de pipeline con los datos de entrenamiento sin procesar (no indexados).\n","# Esta única llamada a '.fit()' ejecuta correctamente todas las etapas en orden.\n","model = pipeline.fit(train_data)\n","\n","# Extraer puntuaciones de importancia de la etapa del modelo RandomForest\n","importances = model.stages[-1].featureImportances.toArray()\n","\n","# Obtener los nombres de las características de la etapa del ensamblador en el modelo al que se aplico '.fit()'\n","feature_names = model.stages[-2].getInputCols()\n","\n","# Crear un DataFrame para mostrar la importancia de las características\n","importance_df = pd.DataFrame(list(zip(feature_names, importances)), columns=['feature', 'importance'])\n","importance_df = importance_df.sort_values('importance', ascending=False)\n","\n","print(\"Las 10 características principales por importancia:\")\n","print(importance_df.head(10))\n","\n","# Seleccionar dinámicamente las 10 características principales para el modelo final\n","top_features = importance_df.head(10)['feature'].tolist()\n","print(f\"\\nCaracterísticas seleccionadas para el modelo final:\\n{top_features}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wr1viZQkq-Mk","executionInfo":{"status":"ok","timestamp":1754762344911,"user_tz":240,"elapsed":2747199,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"outputId":"19c36bdb-c358-40cb-f4b7-40287c4ebaf7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Las 10 características principales por importancia:\n","                  feature  importance\n","0            log_duration    0.453051\n","9         day_of_week_cos    0.189940\n","1   haversine_distance_km    0.147129\n","4        trip_day_of_week    0.103232\n","6                hour_sin    0.025357\n","10         season_encoded    0.015699\n","5              trip_month    0.010721\n","3               trip_hour    0.007126\n","8         day_of_week_sin    0.006529\n","2        is_popular_start    0.006483\n","\n","Características seleccionadas para el modelo final:\n"," ['log_duration', 'day_of_week_cos', 'haversine_distance_km', 'trip_day_of_week', 'hour_sin', 'season_encoded', 'trip_month', 'trip_hour', 'day_of_week_sin', 'is_popular_start']\n"]}]},{"cell_type":"code","source":["# =============================================================================\n","# Reevaluación del modelo con características seleccionadas\n","# =============================================================================\n","# Este bloque utiliza las características principales para entrenar y evaluar un modelo de regresión logística final más eficiente.\n","# Ahora separa correctamente la transformación de características del ajuste del modelo para corregir el error.\n","\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml import Pipeline, PipelineModel\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","import pandas as pd\n","\n","# --- Paso 1: Preparar datos utilizando transformadores del primer pipeline ---\n","\n","# El primer pipeline ('modelo') contiene todos los transformadores de características necesarios (indexadores, codificadores).\n","# Se creará un nuevo PipelineModel que contenga SOLO estas etapas de características, excluyendo la etapa final de RandomForestClassifier.\n","# model.stages[:-1] selecciona todas las etapas excepto la última.\n","feature_transformer_pipeline = PipelineModel(stages=model.stages[:-1])\n","\n","# Ahora, se transforma los datos de entrenamiento y prueba para crear todas las columnas de características diseñadas\n","# sin agregar una columna de \"predicción\".\n","transformed_train_data = feature_transformer_pipeline.transform(train_data)\n","transformed_test_data = feature_transformer_pipeline.transform(test_data)\n","\n","# --- Paso 2: Construcción dinámica del pipeline para el modelo final ---\n","\n","print(f\"Reconstruyendo el pipeline con las siguientes características:\\n {top_features}\\n\")\n","\n","# Identificar dinámicamente las características finales para el ensamblador.\n","# La lista 'top_features' ya contiene los nombres correctos de las columnas codificadas.\n","final_assembler_cols = top_features\n","\n","# Comenzar a construir el pipeline final.\n","# NOTA: NO necesita un indexador de etiquetas.\n","# El modelo de regresión logística funcionará en las columnas \"label\" y \"final_features\".\n","final_pipeline_stages = []\n","\n","# 1. Ensamblar solo las características principales seleccionadas en un nuevo vector de características\n","assembler_final = VectorAssembler(inputCols=final_assembler_cols, outputCol=\"final_features\")\n","final_pipeline_stages.append(assembler_final)\n","\n","# 2. Definir el modelo final (Regresión logística)\n","lr = LogisticRegression(labelCol=\"label\", featuresCol=\"final_features\", maxIter=100)\n","final_pipeline_stages.append(lr)\n","\n","# Crear el objeto pipeline_final\n","pipeline_final = Pipeline(stages=final_pipeline_stages)\n","\n","# --- Paso 3: Entrenamiento y evaluación de modelos ---\n","\n","# Entrenar el nuevo pipeline, más enfocado, en los datos de entrenamiento transformados\n","final_model = pipeline_final.fit(transformed_train_data)\n","\n","# Realizar predicciones sobre el conjunto de pruebas transformado\n","predictions = final_model.transform(transformed_test_data)\n","\n","# --- Paso 4: Métricas de rendimiento ---\n","\n","evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n","accuracy = evaluator_accuracy.evaluate(predictions)\n","\n","print(f\"Nueva precisión del modelo: {accuracy:.4f}\")\n","print(f\"Precisión de referencia: 0.7524\")\n","improvement = accuracy - 0.7524\n","print(f\"Mejora del modelo: {improvement:+.4f}\\n\")\n","\n","# MulticlassMetrics para la matriz de confusión y el informe de clasificación\n","predictionAndLabels = predictions.select('prediction', 'label').rdd.map(lambda r: (float(r.prediction), float(r.label)))\n","metrics = MulticlassMetrics(predictionAndLabels)\n","\n","# Matriz de confusión\n","confusion_matrix = metrics.confusionMatrix().toArray()\n","class_labels = model.stages[0].labels # Obtener etiquetas de clase del primer pipeline\n","report_df = pd.DataFrame(confusion_matrix,\n","                         index=[f'Actual: {l}' for l in class_labels],\n","                         columns=[f'Predicted: {l}' for l in class_labels])\n","\n","print(\"Matriz de confusión:\")\n","print(report_df)\n","\n","# Determinar la etiqueta de clase positiva para el informe\n","label_map = {i: l for i, l in enumerate(class_labels)}\n","positive_class_label = 1.0 if label_map.get(1.0) == 'member' else 0.0\n","\n","print(\"\\nInforme de clasificación (para la clase 'member'):\")\n","print(f\"Precision: {metrics.precision(positive_class_label):.4f}\")\n","print(f\"Recall: {metrics.recall(positive_class_label):.4f}\")\n","print(f\"F1-Score: {metrics.fMeasure(positive_class_label):.4f}\\n\")\n","\n","# Desconservar los datos almacenados en caché\n","train_data.unpersist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhAGJuO_tJrB","executionInfo":{"status":"ok","timestamp":1754759133538,"user_tz":240,"elapsed":131314,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"outputId":"f2d18752-a790-4b73-8113-0e1d4bee086a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Reconstruyendo el pipeline con las siguientes características:\n"," ['log_duration', 'day_of_week_cos', 'haversine_distance_km', 'trip_day_of_week', 'hour_sin', 'season_encoded', 'trip_hour', 'hour_cos', 'is_popular_start', 'day_of_week_sin']\n","\n","Nueva precisión del modelo: 0.7856\n","Precisión de referencia: 0.7524\n","Mejora del modelo: +0.0332\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Matriz de confusión:\n","                Predicted: member  Predicted: casual\n","Actual: member           374182.0             9046.0\n","Actual: casual            96227.0            11558.0\n","\n","Informe de clasificación (para la clase 'member'):\n","Precision: 0.7954\n","Recall: 0.9764\n","F1-Score: 0.8767\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["DataFrame[start_station_id: string, ride_id: string, started_at: timestamp, ended_at: timestamp, start_station_name: string, start_lat: double, start_lng: double, end_station_id: string, end_station_name: string, end_lat: double, end_lng: double, member_casual: string, duration_sec: bigint, schema_version: string, periodo: string, log_duration: double, trip_year: int, trip_month: int, trip_day_of_week: int, trip_hour: int, is_weekend: int, season: string, hour_sin: double, hour_cos: double, day_of_week_sin: double, day_of_week_cos: double, haversine_distance_km: double, is_popular_start: int]"]},"metadata":{},"execution_count":9}]}]}