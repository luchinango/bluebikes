{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["FsuxfpCRUYvP","I1RfD8c8oVZx","G5ZXHq2ESO14","yV5KiGnmS4Pb","CBSIhyiDTAID","1wdQsZJzKNcO"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Cargado de archivos**"],"metadata":{"id":"FsuxfpCRUYvP"}},{"cell_type":"code","source":["# =============================================================================\n","# Configuración optimizada de la sesión de Spark\n","# =============================================================================\n","# Esta configuración es más robusta para conjuntos de datos grandes.\n","# Debe ejecutarse una vez al inicio del notebook.\n","\n","from pyspark.sql import SparkSession\n","import os\n","\n","spark = SparkSession.builder \\\n","    .appName(\"BlueBikes-Project\") \\\n","    .config(\"spark.driver.memory\", \"8g\") \\\n","    .config(\"spark.executor.memory\", \"8g\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n","    .config(\"spark.network.timeout\", \"800s\") \\\n","    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n","    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n","    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Establecer variables de entorno para que PySpark funcione correctamente en algunos entornos\n","import sys\n","# os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n","# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n","os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n","os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\""],"metadata":{"id":"eJFdZIdtnr90","executionInfo":{"status":"ok","timestamp":1755394754505,"user_tz":240,"elapsed":12040,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# =============================================================================\n","# Cargar DataFrame procesado desde Google Drive (para Colab)\n","# =============================================================================\n","# Esta celda activa tu Google Drive y lee los datos procesados del archivo Parquet almacenado allí.\n","# Usarlo en el entorno de Google Colab.\n","\n","from google.colab import drive\n","\n","print(\"Intentando montar Google Drive...\")\n","try:\n","    drive.mount('/content/drive')\n","\n","    gdrive_path = \"/content/drive/MyDrive/BlueBikes_PRJ/df_final_bluebikes_v1.parquet\"\n","    print(f\"Cargando datos desde la ruta de Google Drive: {gdrive_path}\")\n","\n","    # Spark leerá la carpeta Parquet directamente\n","    df_final = spark.read.parquet(gdrive_path)\n","\n","    print(\"✅ DataFrame cargado exitosamente desde Google Drive.\")\n","\n","    # Verify the schema and show a few rows\n","    print(\"Esquema de DataFrame:\")\n","    df_final.printSchema()\n","\n","    print(\"Muestra de los datos cargados:\")\n","    df_final.show(5, truncate=False)\n","\n","except Exception as e:\n","    print(f\"❌ Error al cargar datos desde Google Drive. Asegúrate de que el archivo exista en '{gdrive_path}'.\")\n","    print(f\"Detalles del error: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fp_97YheUd1j","executionInfo":{"status":"ok","timestamp":1755394771183,"user_tz":240,"elapsed":16684,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"outputId":"393dafef-72c0-44df-b131-dd8007ca2ddd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Intentando montar Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Cargando datos desde la ruta de Google Drive: /content/drive/MyDrive/BlueBikes_PRJ/df_final_bluebikes_v1.parquet\n","✅ DataFrame cargado exitosamente desde Google Drive.\n","Esquema de DataFrame:\n","root\n"," |-- ride_id: string (nullable = true)\n"," |-- rideable_type: string (nullable = true)\n"," |-- started_at: timestamp (nullable = true)\n"," |-- ended_at: timestamp (nullable = true)\n"," |-- start_station_name: string (nullable = true)\n"," |-- start_lat: double (nullable = true)\n"," |-- start_lng: double (nullable = true)\n"," |-- end_station_name: string (nullable = true)\n"," |-- end_lat: double (nullable = true)\n"," |-- end_lng: double (nullable = true)\n"," |-- member_casual: string (nullable = true)\n"," |-- duration_sec: long (nullable = true)\n"," |-- schema_version: string (nullable = true)\n"," |-- periodo: string (nullable = true)\n","\n","Muestra de los datos cargados:\n","+----------------+-------------+-----------------------+-----------------------+---------------------------------------------+------------------+------------------+-----------------------------------------------------+------------------+------------------+-------------+------------+--------------+-------+\n","|ride_id         |rideable_type|started_at             |ended_at               |start_station_name                           |start_lat         |start_lng         |end_station_name                                     |end_lat           |end_lng           |member_casual|duration_sec|schema_version|periodo|\n","+----------------+-------------+-----------------------+-----------------------+---------------------------------------------+------------------+------------------+-----------------------------------------------------+------------------+------------------+-------------+------------+--------------+-------+\n","|063BFCFD9F16B545|classic_bike |2024-09-29 09:33:59.648|2024-09-29 09:38:54.194|MLK Blvd at Washington St                    |42.32143814183195 |-71.09126061201096|NCAAA - Walnut Ave at Crawford St                    |42.316902         |-71.091946        |member       |295         |schema2       |202409 |\n","|80C892E9306C7CD7|classic_bike |2024-09-11 17:55:35.235|2024-09-11 18:10:33.662|Kendall T                                    |42.362427842912396|-71.08495473861694|Conway Park - Somerville Avenue                      |42.383638948222654|-71.10853672027588|member       |898         |schema2       |202409 |\n","|F58DE18F42773C91|classic_bike |2024-09-12 16:53:38.718|2024-09-12 17:10:49.625|One Broadway / Kendall Sq at Main St / 3rd St|42.36224178650923 |-71.0831107199192 |Mass Ave T Station                                   |42.34135615767354 |-71.08336953450817|member       |1031        |schema2       |202409 |\n","|7DB07D9D4D1921F4|electric_bike|2024-09-15 18:36:23.819|2024-09-15 18:43:14.393|MLK Blvd at Washington St                    |42.32143814183195 |-71.09126061201096|Dudley Town Common - Mt Pleasant Ave at Blue Hill Ave|42.325333         |-71.075354        |casual       |411         |schema2       |202409 |\n","|C8CF62DC077ABBE2|classic_bike |2024-09-04 15:15:11.68 |2024-09-04 15:23:11.451|One Broadway / Kendall Sq at Main St / 3rd St|42.36224178650923 |-71.0831107199192 |Nashua Street at Red Auerbach Way                    |42.365673         |-71.064263        |casual       |480         |schema2       |202409 |\n","+----------------+-------------+-----------------------+-----------------------+---------------------------------------------+------------------+------------------+-----------------------------------------------------+------------------+------------------+-------------+------------+--------------+-------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["# **Ingeniería de Características**"],"metadata":{"id":"UhyBvKlbJqld"}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n","\n","# Asegura tipos y una columna de fecha base para los flags\n","df_final = (\n","    df_final\n","    .withColumn(\"start_lat\", F.col(\"start_lat\").cast(\"double\"))\n","    .withColumn(\"start_lng\", F.col(\"start_lng\").cast(\"double\"))\n","    .withColumn(\"end_lat\",   F.col(\"end_lat\").cast(\"double\"))\n","    .withColumn(\"end_lng\",   F.col(\"end_lng\").cast(\"double\"))\n","    .withColumn(\"started_at\", F.to_timestamp(\"started_at\"))\n","    .withColumn(\"ended_at\",   F.to_timestamp(\"ended_at\"))\n",")"],"metadata":{"id":"Ouz1mRd8JwDM","executionInfo":{"status":"ok","timestamp":1755394771472,"user_tz":240,"elapsed":288,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Conversión a Zona Horaria de Boston (hora y truncado a hora)"],"metadata":{"id":"I1RfD8c8oVZx"}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n","\n","# asegurar tipo timestamp\n","df_final = (df_final\n","    .withColumn(\"started_at\", F.to_timestamp(\"started_at\"))\n","    .withColumn(\"ended_at\",   F.to_timestamp(\"ended_at\"))\n",")\n","\n","# === A) si tus timestamps están en UTC → conviértelos a Boston ===\n","df_final = df_final.withColumn(\n","    \"started_at_local\",\n","    F.from_utc_timestamp(\"started_at\", \"America/New_York\")\n",")\n","# === B) si YA están en hora local, usa esta en lugar de A\n","df_final = (df_final\n","    .withColumn(\"ts_hour\", F.date_trunc(\"hour\", F.col(\"started_at_local\")))  # inicio de la hora\n",")"],"metadata":{"id":"eEtM0lDaoWkn","executionInfo":{"status":"ok","timestamp":1755394771638,"user_tz":240,"elapsed":164,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Ingeniería de características temporales"],"metadata":{"id":"G5ZXHq2ESO14"}},{"cell_type":"code","source":["# Extraeremos varias características basadas en el tiempo de la marca de tiempo 'started_at'.\n","# Esto ayudará al modelo a capturar patrones relacionados con la hora del día, la semana y el año.\n","\n","from pyspark.sql.functions import col, year, month, dayofweek, hour, dayofyear, weekofyear, when, sin, cos, radians, date_format\n","import numpy as np\n","\n","# Extraer características temporales básicas\n","df_with_temporal = df_final \\\n","    .withColumn('trip_date', col('started_at_local').cast('date')) \\\n","    .withColumn('trip_year', year(col('started_at_local'))) \\\n","    .withColumn('trip_month', month(col('started_at_local'))) \\\n","    .withColumn('trip_day_of_week', dayofweek(col('started_at_local'))) \\\n","    .withColumn('trip_hour', hour(col('started_at_local'))) \\\n","    .withColumn('is_weekend', when(col('trip_day_of_week').isin([1, 7]), 1).otherwise(0))\n","\n","# Definir estaciones según los meses\n","df_with_temporal = df_with_temporal.withColumn(\n","    'season',\n","    when(col('trip_month').isin([12, 1, 2]), 'winter')\n","    .when(col('trip_month').isin([3, 4, 5]), 'spring')\n","    .when(col('trip_month').isin([6, 7, 8]), 'summer')\n","    .otherwise('fall')\n",")\n","\n","# Codificar características cíclicas (hora y día de la semana) mediante transformaciones de seno y coseno.\n","# Esto ayuda al modelo a comprender la naturaleza continua del tiempo.\n","# Ciclo horario (24 horas)\n","df_with_temporal = df_with_temporal \\\n","    .withColumn('hour_sin', sin(2 * np.pi * col('trip_hour') / 24)) \\\n","    .withColumn('hour_cos', cos(2 * np.pi * col('trip_hour') / 24))\n","\n","# Ciclo de día de la semana (7 días)\n","df_with_temporal = df_with_temporal \\\n","    .withColumn('day_of_week_sin', sin(2 * np.pi * col('trip_day_of_week') / 7)) \\\n","    .withColumn('day_of_week_cos', cos(2 * np.pi * col('trip_day_of_week') / 7))\n","\n","# Actualizar el DataFrame principal\n","df_final = df_with_temporal"],"metadata":{"id":"BJnbrJoSSZ1R","executionInfo":{"status":"ok","timestamp":1755394772318,"user_tz":240,"elapsed":679,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Ingeniería de características geoespaciales"],"metadata":{"id":"yV5KiGnmS4Pb"}},{"cell_type":"code","source":["# Crearemos nuevas funciones basadas en las coordenadas de la estación para capturar patrones geoespaciales.\n","# La distancia de Haversine es una función clave para predecir la duración del viaje.\n","# También crearemos funciones relacionadas con la popularidad/densidad de la estación.\n","\n","from pyspark.sql.functions import udf, lit, count\n","from pyspark.sql.types import DoubleType\n","from math import radians, sin, cos, sqrt, atan2\n","\n","# Distancia de Haversina UDF\n","def haversine(lon1, lat1, lon2, lat2):\n","    R = 6371.0 # Radio de la Tierra en km\n","\n","    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n","\n","    dlon = lon2 - lon1\n","    dlat = lat2 - lat1\n","\n","    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n","    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n","\n","    distance = R * c\n","    return distance\n","\n","haversine_udf = udf(haversine, DoubleType())\n","\n","# Calcular la distancia haversina entre las estaciones inicial y final\n","df_with_geospatial = df_final.withColumn(\n","    'haversine_distance_km',\n","    haversine_udf(\n","        col('start_lng'), col('start_lat'),\n","        col('end_lng'), col('end_lat')\n","    )\n",")\n","\n","# Función: Calcular indicadores de \"estación popular\" según el historial de viajes.\n","# Definiremos una estación como popular si su total de viajes se encuentra en el 10% superior.\n","trip_counts_by_station = df_with_geospatial.groupBy('start_station_name').agg(count('*').alias('trip_count'))\n","percentile_threshold = trip_counts_by_station.approxQuantile('trip_count', [0.90], 0.05)[0]\n","\n","popular_stations = trip_counts_by_station.filter(col('trip_count') > percentile_threshold) \\\n","    .select('start_station_name').withColumn('is_popular_start', lit(1))\n","\n","# Une el flag binario de la estación popular al DataFrame principal\n","df_with_geospatial = df_with_geospatial.join(\n","    popular_stations,\n","    on='start_station_name',\n","    how='left'\n",").fillna(0, subset=['is_popular_start'])\n","\n","# Actualizar el DataFrame principal\n","df_final = df_with_geospatial"],"metadata":{"id":"P4IULZEJfTIW","executionInfo":{"status":"ok","timestamp":1755394790139,"user_tz":240,"elapsed":17816,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Ingeniería de características de interacción"],"metadata":{"id":"CBSIhyiDTAID"}},{"cell_type":"code","source":["# Crearemos características de interacción que capturen relaciones entre variables existentes,\n","# como la velocidad del viaje y si el viaje fue de ida y vuelta.\n","\n","from pyspark.sql.functions import col, when\n","\n","# Crear una característica para la velocidad promedio en km/h\n","# Se añade una pequeña constante (1e-6) al denominador para evitar la división por cero,\n","df_with_interaction_features = df_final.withColumn(\n","    \"avg_speed_kmh\",\n","    col(\"haversine_distance_km\") / (col(\"duration_sec\") / 3600 + 1e-6)\n",")\n","\n","# Crear una característica binaria para viajes de ida y vuelta (misma estación de inicio y fin)\n","df_with_interaction_features = df_with_interaction_features.withColumn(\n","    \"is_round_trip\",\n","    when(col(\"start_station_name\") == col(\"end_station_name\"), 1).otherwise(0)\n",")\n","\n","\n","# Actualizar el DataFrame principal para incluir estas nuevas características\n","df_final = df_with_interaction_features"],"metadata":{"id":"GTtL4a9uTGG1","executionInfo":{"status":"ok","timestamp":1755394790280,"user_tz":240,"elapsed":139,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Feriados Massachusetts (USA): is_holiday"],"metadata":{"id":"1wdQsZJzKNcO"}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n","import datetime as dt\n","\n","# A) rango de años presentes\n","mm = df_final.agg(F.min(\"trip_date\").alias(\"min_d\"), F.max(\"trip_date\").alias(\"max_d\")).first()\n","years = list(range(mm[\"min_d\"].year, mm[\"max_d\"].year + 1))\n","\n","# B) construir calendario de feriados (Massachusetts)\n","try:\n","    import holidays\n","except ImportError as e:\n","    raise RuntimeError(\"Instala el paquete 'holidays' con: pip install holidays\") from e\n","\n","us_ma = holidays.US(subdiv=\"MA\", years=years)  # incluye 'observed'\n","holiday_dates = sorted(us_ma.keys())\n","\n","# C) tabla de feriados -> Spark\n","holiday_df = (\n","    spark.createDataFrame([(d.isoformat(),) for d in holiday_dates], [\"date_str\"])\n","         .select(F.to_date(\"date_str\").alias(\"trip_date\"))\n","         .withColumn(\"is_holiday\", F.lit(True))\n",")\n","\n","# D) unir al hecho por fecha\n","df_final = (\n","    df_final\n","    .join(holiday_df, on=\"trip_date\", how=\"left\")\n","    .withColumn(\"is_holiday\", F.coalesce(\"is_holiday\", F.lit(False)).cast(\"integer\")) # Cast boolean a entero (0 o 1)\n",")"],"metadata":{"id":"nN7uyLjJKPE2","executionInfo":{"status":"ok","timestamp":1755394944604,"user_tz":240,"elapsed":32161,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Columnas creadas"],"metadata":{"id":"eSZLYICSL8wN"}},{"cell_type":"code","source":["# Lista de columnas originales basadas en el esquema\n","original_cols = [\n","    'ride_id', 'rideable_type', 'started_at', 'ended_at', 'start_station_name',\n","    'start_lat', 'start_lng', 'end_station_name', 'end_lat', 'end_lng',\n","    'member_casual', 'duration_sec', 'schema_version', 'periodo'\n","]\n","\n","# Obtener la lista de todas las columnas en el DataFrame final\n","all_cols = df_final.columns\n","\n","# Identificar las columnas creadas por la Ing. de Características excluyendo las columnas originales\n","feature_engineered_cols = [col for col in all_cols if col not in original_cols]\n","\n","# Conteo del número de columnas agregadas por la Ing. de Características\n","num_feature_engineered_cols = len(feature_engineered_cols)\n","print(f\"Cantidad de columnas agregadas por la Ing. de Características: {num_feature_engineered_cols}\")\n","\n","# Seleccionar y mostrar solo las columnas creadas por la Ing. de Características\n","df_final.select(feature_engineered_cols).show(5, truncate=False)"],"metadata":{"id":"xSKa_SrlMAuB","executionInfo":{"status":"ok","timestamp":1755395721126,"user_tz":240,"elapsed":14165,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8123d716-4bd6-42db-b162-695eb1ae0acf"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Cantidad de columnas agregadas por la Ing. de Características: 18\n","+----------+-----------------------+-------------------+---------+----------+----------------+---------+----------+------+----------------------+-------------------+-------------------+--------------------+---------------------+----------------+------------------+-------------+----------+\n","|trip_date |started_at_local       |ts_hour            |trip_year|trip_month|trip_day_of_week|trip_hour|is_weekend|season|hour_sin              |hour_cos           |day_of_week_sin    |day_of_week_cos     |haversine_distance_km|is_popular_start|avg_speed_kmh     |is_round_trip|is_holiday|\n","+----------+-----------------------+-------------------+---------+----------+----------------+---------+----------+------+----------------------+-------------------+-------------------+--------------------+---------------------+----------------+------------------+-------------+----------+\n","|2024-09-29|2024-09-29 05:33:59.648|2024-09-29 05:00:00|2024     |9         |1               |5        |1         |fall  |0.9659258262890683    |0.25881904510252074|0.7818314824680298 |0.6234898018587336  |0.5075339967500118   |0               |6.193559632153785 |0            |0         |\n","|2024-09-15|2024-09-15 14:36:23.819|2024-09-15 14:00:00|2024     |9         |1               |14       |1         |fall  |-0.4999999999999997   |-0.8660254037844388|0.7818314824680298 |0.6234898018587336  |1.377574161395623    |0               |12.066237329853662|0            |0         |\n","|2024-09-12|2024-09-12 12:53:38.718|2024-09-12 12:00:00|2024     |9         |5               |12       |0         |fall  |1.2246467991473532E-16|-1.0               |-0.9749279121818236|-0.2225209339563146 |2.322473351059437    |1               |8.109480959924847 |0            |0         |\n","|2024-09-04|2024-09-04 11:15:11.68 |2024-09-04 11:00:00|2024     |9         |4               |11       |0         |fall  |0.258819045102521     |-0.9659258262890682|-0.433883739117558 |-0.9009688679024191 |1.594831561431778    |1               |11.961147002135819|0            |0         |\n","|2024-09-09|2024-09-09 03:15:15.312|2024-09-09 03:00:00|2024     |9         |2               |3        |0         |fall  |0.7071067811865475    |0.7071067811865476 |0.9749279121818236 |-0.22252093395631434|1.6208846037115126   |1               |5.214625380437958 |0            |0         |\n","+----------+-----------------------+-------------------+---------+----------+----------------+---------+----------+------+----------------------+-------------------+-------------------+--------------------+---------------------+----------------+------------------+-------------+----------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["# **Guardado de datos a formato PARQUET**"],"metadata":{"id":"fvnOaSHKe5SP"}},{"cell_type":"code","source":["# Guardar el DataFrame limpio en un archivo Parquet es un paso crucial.\n","# Conserva el trabajo, lo que  permite comenzar la fase de Ingeniería de Características y\n","# Modelado en un cuaderno aparte sin tener que volver a ejecutar todo el proceso.\n","# Esto ahorra mucho tiempo y recursos computacionales.\n","\n","from pyspark.sql.functions import col\n","\n","# --- Definir ruta de Google Drive ---\n","\n","# Asegurar de crear una carpeta llamada 'BlueBikes_PRJ' en el Drive.\n","gdrive_path = \"/content/drive/MyDrive/BlueBikes_PRJ/df_final_bluebikes_v2.parquet\"\n","\n","# Esta parte del código solo se ejecutará si detecta que está en un entorno de Google Colab.\n","try:\n","    from google.colab import drive\n","\n","    print(\"\\nIntentando montar Google Drive...\")\n","    drive.mount('/content/drive')\n","\n","    print(f\"Intentando guardar DataFrame en Google Drive: {gdrive_path}\")\n","\n","    # Al escribir en un sistema de archivos distribuido como Google Drive,\n","    # suele ser mejor dejar que Spark administre las particiones.\n","    # No usaremos coalesce(1) aquí para mantener el paralelismo.\n","    df_final.write.mode(\"overwrite\").parquet(gdrive_path)\n","\n","    print(f\"✅ DataFrame se guardó correctamente en Google Drive.\")\n","\n","except ImportError:\n","    # Esto se activará si no se encuentra 'google.colab',\n","    # lo que significa que probablemente se tiene un entorno local.\n","    print(\"\\nOmitir el guardado de Google Drive: no se ejecuta en un entorno de Google Colab.\")\n","except Exception as e:\n","    print(f\"❌ Se produjo un error al guardar en Google Drive: {e}\")"],"metadata":{"id":"4lJ6Dp5Be62q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755395685355,"user_tz":240,"elapsed":634770,"user":{"displayName":"Joseph Thenier","userId":"13761088060173374604"}},"outputId":"086b810b-ea8e-44aa-d775-56362f29205e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Intentando montar Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Intentando guardar DataFrame en Google Drive: /content/drive/MyDrive/BlueBikes_PRJ/df_final_bluebikes_v2.parquet\n","✅ DataFrame se guardó correctamente en Google Drive.\n"]}]}]}